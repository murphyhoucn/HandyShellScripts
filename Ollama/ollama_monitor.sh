#!/bin/bash

# Ollama è¿è¡ŒçŠ¶æ€ç›‘æ§è„šæœ¬
# ç”¨äºå®æ—¶ç›‘æ§OllamaæœåŠ¡å’Œæ¨¡å‹è¿è¡ŒçŠ¶æ€

SERVER_IP="x.x.x.x"  # è¯·æ›¿æ¢ä¸ºä½ çš„æœåŠ¡å™¨IPåœ°å€
OLLAMA_PORT="x"  # è¯·æ›¿æ¢ä¸ºä½ çš„Ollamaç«¯å£

# é…ç½®
OLLAMA_API="http://$SERVER_IP:$OLLAMA_PORT"
REFRESH_INTERVAL=5  # åˆ·æ–°é—´éš”ï¼ˆç§’ï¼‰

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
PURPLE='\033[0;35m'
NC='\033[0m' # No Color

# æ¸…å±å¹¶æ˜¾ç¤ºæ ‡é¢˜
show_header() {
    clear
    echo -e "${BLUE}================================================${NC}"
    echo -e "${BLUE}          Ollama è¿è¡ŒçŠ¶æ€ç›‘æ§é¢æ¿${NC}"
    echo -e "${BLUE}================================================${NC}"
    echo -e "ç›‘æ§åœ°å€: ${GREEN}${OLLAMA_API}${NC}"
    echo -e "åˆ·æ–°é—´éš”: ${GREEN}${REFRESH_INTERVAL}ç§’${NC} (æŒ‰ Ctrl+C é€€å‡º)"
    echo ""
}

# è·å–æœåŠ¡åŸºæœ¬çŠ¶æ€
get_service_status() {
    echo -e "${CYAN}=== æœåŠ¡çŠ¶æ€ ===${NC}"
    
    # æ£€æŸ¥systemdæœåŠ¡çŠ¶æ€
    if systemctl is-active --quiet ollama; then
        echo -e "ğŸŸ¢ OllamaæœåŠ¡: ${GREEN}è¿è¡Œä¸­${NC}"
        local uptime=$(systemctl show ollama --property=ActiveEnterTimestamp --value)
        echo -e "ğŸ“… å¯åŠ¨æ—¶é—´: ${GREEN}$(date -d "$uptime" '+%Y-%m-%d %H:%M:%S')${NC}"
    else
        echo -e "ğŸ”´ OllamaæœåŠ¡: ${RED}æœªè¿è¡Œ${NC}"
        return 1
    fi
    
    # æ£€æŸ¥è¿›ç¨‹
    local main_pid=$(pgrep -f "ollama serve")
    local runner_pids=$(pgrep -f "ollama runner")
    
    if [ -n "$main_pid" ]; then
        echo -e "ğŸ”§ ä¸»è¿›ç¨‹PID: ${GREEN}$main_pid${NC}"
    fi
    
    if [ -n "$runner_pids" ]; then
        echo -e "âš¡ è¿è¡Œå™¨è¿›ç¨‹: ${GREEN}$(echo $runner_pids | wc -w)ä¸ª${NC}"
    fi
    
    echo ""
}

# è·å–GPUä½¿ç”¨æƒ…å†µ
get_gpu_status() {
    echo -e "${CYAN}=== GPU ä½¿ç”¨çŠ¶æ€ ===${NC}"
    
    if command -v nvidia-smi &> /dev/null; then
        # è·å–GPU 2çš„ä¿¡æ¯ï¼ˆOllama Config, Ref /etc/systemd/system/ollama.service.d/ï¼‰
        local gpu_info=$(nvidia-smi --query-gpu=index,name,temperature.gpu,power.draw,memory.used,memory.total,utilization.gpu --format=csv,noheader,nounits | sed -n '3p')
        
        if [ -n "$gpu_info" ]; then
            IFS=',' read -r index name temp power mem_used mem_total util <<< "$gpu_info"
            
            echo -e "ğŸ® GPU ${index} (${name// /})"
            echo -e "ğŸŒ¡ï¸  æ¸©åº¦: ${GREEN}${temp// /}Â°C${NC}"
            echo -e "âš¡ åŠŸè€—: ${GREEN}${power// /}W${NC}"
            echo -e "ğŸ’¾ æ˜¾å­˜: ${GREEN}${mem_used// /}MB${NC} / ${mem_total// /}MB ($(( (${mem_used// /} * 100) / ${mem_total// /} ))%)"
            echo -e "ğŸ“Š åˆ©ç”¨ç‡: ${GREEN}${util// /}%${NC}"
        fi
    else
        echo -e "${YELLOW}âš ï¸  nvidia-smi æœªå®‰è£…${NC}"
    fi
    
    echo ""
}

# è·å–å½“å‰è¿è¡Œçš„æ¨¡å‹
get_running_models() {
    echo -e "${CYAN}=== å½“å‰è¿è¡Œæ¨¡å‹ ===${NC}"
    
    local response=$(curl -s --connect-timeout 3 "${OLLAMA_API}/api/ps" 2>/dev/null)
    
    if [ $? -eq 0 ] && [ -n "$response" ]; then
        local models=$(echo "$response" | jq -r '.models[]? | "\(.name)|\(.size)|\(.size_vram)|\(.expires_at)|\(.context_length)"' 2>/dev/null)
        
        if [ -n "$models" ] && [ "$models" != "null" ]; then
            echo "$models" | while IFS='|' read -r name size size_vram expires_at context_length; do
                echo -e "ğŸ¤– æ¨¡å‹: ${GREEN}$name${NC}"
                echo -e "ğŸ“¦ æ¨¡å‹å¤§å°: ${GREEN}$(( size / 1024 / 1024 ))MB${NC}"
                echo -e "ğŸ’¾ æ˜¾å­˜å ç”¨: ${GREEN}$(( size_vram / 1024 / 1024 ))MB${NC}"
                echo -e "â° è¿‡æœŸæ—¶é—´: ${GREEN}$(date -d "$expires_at" '+%H:%M:%S' 2>/dev/null || echo "N/A")${NC}"
                echo -e "ğŸ“ ä¸Šä¸‹æ–‡é•¿åº¦: ${GREEN}$context_length${NC}"
                echo ""
            done
        else
            echo -e "${YELLOW}ğŸ“­ å½“å‰æ²¡æœ‰è¿è¡Œä¸­çš„æ¨¡å‹${NC}"
            echo ""
        fi
    else
        echo -e "${RED}âŒ æ— æ³•è¿æ¥åˆ°Ollama API${NC}"
        echo ""
    fi
}

# è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
get_available_models() {
    echo -e "${CYAN}=== å¯ç”¨æ¨¡å‹åˆ—è¡¨ ===${NC}"
    
    local response=$(curl -s --connect-timeout 3 "${OLLAMA_API}/api/tags" 2>/dev/null)
    
    if [ $? -eq 0 ] && [ -n "$response" ]; then
        local models=$(echo "$response" | jq -r '.models[]? | "\(.name)|\(.size)|\(.modified_at)"' 2>/dev/null)
        
        if [ -n "$models" ] && [ "$models" != "null" ]; then
            echo "$models" | while IFS='|' read -r name size modified_at; do
                echo -e "ğŸ“š ${GREEN}$name${NC} ($(( size / 1024 / 1024 ))MB)"
            done
        else
            echo -e "${YELLOW}ğŸ“­ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨æ¨¡å‹${NC}"
        fi
    else
        echo -e "${RED}âŒ æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨${NC}"
    fi
    
    echo ""
}

# è·å–ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
get_system_resources() {
    echo -e "${CYAN}=== ç³»ç»Ÿèµ„æº ===${NC}"
    
    # CPUä½¿ç”¨ç‡
    local cpu_usage=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d'%' -f1)
    echo -e "ğŸ–¥ï¸  CPUä½¿ç”¨ç‡: ${GREEN}${cpu_usage}%${NC}"
    
    # å†…å­˜ä½¿ç”¨ç‡
    local mem_info=$(free | grep Mem)
    local mem_used=$(echo "$mem_info" | awk '{print $3}')
    local mem_total=$(echo "$mem_info" | awk '{print $2}')
    local mem_percent=$(( (mem_used * 100) / mem_total ))
    echo -e "ğŸ’» å†…å­˜ä½¿ç”¨ç‡: ${GREEN}${mem_percent}%${NC} ($(( mem_used / 1024 / 1024 ))GB / $(( mem_total / 1024 / 1024 ))GB)"
    
    # ç£ç›˜ä½¿ç”¨æƒ…å†µ
    local disk_usage=$(df -h /usr/share/ollama 2>/dev/null | tail -1 | awk '{print $5}' | cut -d'%' -f1)
    if [ -n "$disk_usage" ]; then
        echo -e "ğŸ’¿ æ¨¡å‹å­˜å‚¨: ${GREEN}${disk_usage}%${NC}"
    fi
    
    echo ""
}

# å®æ—¶ç›‘æ§æ¨¡å¼
monitor_mode() {
    while true; do
        show_header
        get_service_status
        get_gpu_status
        get_running_models
        get_available_models
        get_system_resources
        
        echo -e "${PURPLE}ä¸‹æ¬¡åˆ·æ–°: $(date -d "+${REFRESH_INTERVAL} seconds" '+%H:%M:%S')${NC}"
        sleep $REFRESH_INTERVAL
    done
}

# å•æ¬¡çŠ¶æ€æ£€æŸ¥
status_check() {
    show_header
    get_service_status
    get_gpu_status
    get_running_models
    get_available_models
    get_system_resources
}

# GPUè¯¦ç»†ä¿¡æ¯
gpu_detail() {
    echo -e "${BLUE}=== GPUè¯¦ç»†ä¿¡æ¯ ===${NC}"
    if command -v nvidia-smi &> /dev/null; then
        nvidia-smi -i 2 --query-gpu=index,name,driver_version,temperature.gpu,power.draw,power.limit,memory.used,memory.total,utilization.gpu,utilization.memory --format=csv
    else
        echo -e "${RED}nvidia-smi æœªå®‰è£…${NC}"
    fi
}

# æ˜¾ç¤ºå¸®åŠ©
show_help() {
    echo "Ollamaç›‘æ§å·¥å…·ä½¿ç”¨è¯´æ˜ï¼š"
    echo ""
    echo "ç”¨æ³•: $0 [é€‰é¡¹]"
    echo ""
    echo "é€‰é¡¹:"
    echo "  monitor, -m     å®æ—¶ç›‘æ§æ¨¡å¼ (é»˜è®¤)"
    echo "  status, -s      å•æ¬¡çŠ¶æ€æ£€æŸ¥"
    echo "  gpu, -g         æ˜¾ç¤ºGPUè¯¦ç»†ä¿¡æ¯"
    echo "  models, -l      åªæ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯"
    echo "  help, -h        æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯"
    echo ""
    echo "ç¯å¢ƒå˜é‡:"
    echo "  OLLAMA_API      Ollama APIåœ°å€ (é»˜è®¤: http://:$OLLAMA_PORT)"
    echo "  REFRESH_INTERVAL åˆ·æ–°é—´éš”ç§’æ•° (é»˜è®¤: 5)"
}

# åªæ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
models_only() {
    get_running_models
    get_available_models
}

# ä¸»ç¨‹åº
main() {
    case "$1" in
        "monitor"|"-m"|"")
            monitor_mode
            ;;
        "status"|"-s")
            status_check
            ;;
        "gpu"|"-g")
            gpu_detail
            ;;
        "models"|"-l")
            models_only
            ;;
        "help"|"-h")
            show_help
            ;;
        *)
            echo "æœªçŸ¥é€‰é¡¹: $1"
            show_help
            exit 1
            ;;
    esac
}

# æ£€æŸ¥ä¾èµ–
if ! command -v jq &> /dev/null; then
    echo -e "${YELLOW}è­¦å‘Š: æœªå®‰è£…jqï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½å—é™${NC}"
    echo "å®‰è£…å‘½ä»¤: sudo apt install jq"
    echo ""
fi

# æ•è·Ctrl+Cä¿¡å·
trap 'echo -e "\n${GREEN}ç›‘æ§å·²åœæ­¢${NC}"; exit 0' INT

# è¿è¡Œä¸»ç¨‹åº
main "$@"